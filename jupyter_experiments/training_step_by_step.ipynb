{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b8e7723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cebaf91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Training script for hallucination detection probes.\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import atexit\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from dataclasses import asdict\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from torch.utils.data import Subset\n",
    "from transformers import TrainingArguments\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from utils.file_utils import save_jsonl, save_json, load_yaml\n",
    "from utils.model_utils import load_model_and_tokenizer, print_trainable_parameters\n",
    "from utils.probe_loader import upload_probe_to_hf\n",
    "\n",
    "from probe.dataset import TokenizedProbingDataset, create_probing_dataset, tokenized_probing_collate_fn\n",
    "from probe.config import TrainingConfig\n",
    "from probe.value_head_probe import setup_probe\n",
    "from probe.trainer import ProbeTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4918a279",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(training_config: TrainingConfig):\n",
    "    \"\"\"Main training function.\"\"\"\n",
    "\n",
    "    if hasattr(model, 'config'):\n",
    "        try:\n",
    "            model.config.use_cache = False\n",
    "        except Exception:\n",
    "            pass\n",
    "    if training_config.enable_gradient_checkpointing and hasattr(model, 'gradient_checkpointing_enable'):\n",
    "        try:\n",
    "            model.gradient_checkpointing_enable()\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    print(f\"Setting up probe: {training_config.probe_config.probe_id}\")\n",
    "    model, probe = setup_probe(model, training_config.probe_config)\n",
    "\n",
    "    print_trainable_parameters(probe)\n",
    "\n",
    "    # Load datasets\n",
    "    print(\"Loading datasets:\")\n",
    "    train_datasets: List[TokenizedProbingDataset] = [\n",
    "        create_probing_dataset(config, tokenizer)\n",
    "        for config in training_config.train_dataset_configs\n",
    "    ]\n",
    "    eval_datasets: List[TokenizedProbingDataset] = [\n",
    "        create_probing_dataset(config, tokenizer)\n",
    "        for config in training_config.eval_dataset_configs\n",
    "    ]\n",
    "    \n",
    "    # Concatenate training datasets\n",
    "    train_dataset = train_datasets[0]\n",
    "    for dataset in train_datasets[1:]:\n",
    "        train_dataset += dataset\n",
    "\n",
    "    # If requested, shuffle and shave down the training dataset to a fixed number of samples\n",
    "    if training_config.num_train_samples is not None:\n",
    "        total = len(train_dataset)\n",
    "        num = max(0, min(int(training_config.num_train_samples), total))\n",
    "        if num < total:\n",
    "            g = torch.Generator()\n",
    "            g.manual_seed(training_config.seed)\n",
    "            perm = torch.randperm(total, generator=g).tolist()\n",
    "            selected_indices = perm[:num]\n",
    "            train_dataset = Subset(train_dataset, selected_indices)\n",
    "            print(f\"Using a subset of the training dataset: {num}/{total} samples\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(training_config.probe_config.probe_path),\n",
    "        overwrite_output_dir=True,\n",
    "        per_device_train_batch_size=training_config.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=training_config.per_device_eval_batch_size,\n",
    "        max_steps=training_config.max_steps,\n",
    "        num_train_epochs=training_config.num_train_epochs,\n",
    "        logging_steps=training_config.logging_steps,\n",
    "        eval_steps=training_config.eval_steps,\n",
    "        remove_unused_columns=False,\n",
    "        label_names=[\"classification_labels\", \"lm_labels\"],\n",
    "        report_to=\"wandb\",\n",
    "        run_name=training_config.probe_config.probe_id,\n",
    "        eval_strategy=\"steps\" if training_config.eval_steps else \"no\",\n",
    "        logging_first_step=True,\n",
    "        logging_strategy=\"steps\",\n",
    "        max_grad_norm=training_config.max_grad_norm,\n",
    "        gradient_accumulation_steps=training_config.gradient_accumulation_steps,\n",
    "        learning_rate=training_config.learning_rate,\n",
    "        seed=training_config.seed,\n",
    "    )\n",
    "    \n",
    "    # Add separate learning rates to training_args\n",
    "    training_args.probe_head_lr = training_config.probe_head_lr\n",
    "    training_args.lora_lr = training_config.lora_lr\n",
    "\n",
    "    # Disable checkpoint saving\n",
    "    # (there's a weird bug that occurs when trying to save during training)\n",
    "    training_args.set_save(strategy=\"no\")\n",
    "\n",
    "    trainer = ProbeTrainer(\n",
    "        probe=probe,\n",
    "        eval_datasets=eval_datasets,\n",
    "        cfg=training_config,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=None, # this is a dummy argument is for the HF base Trainer class\n",
    "        data_collator=tokenized_probing_collate_fn,\n",
    "        eval_steps=training_config.eval_steps,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    def save_model_callback():\n",
    "        \"\"\"Save probe weigths, tokenizer and training config to disk.\"\"\"\n",
    "        probe.save(training_config.probe_config.probe_path)\n",
    "        tokenizer.save_pretrained(training_config.probe_config.probe_path)\n",
    "        save_json(\n",
    "            training_config,\n",
    "            training_config.probe_config.probe_path / \"training_config.json\"\n",
    "        )\n",
    "\n",
    "    # Register save callback for unexpected exits\n",
    "    atexit.register(save_model_callback)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the model\n",
    "    print(f\"Saving model to {training_config.probe_config.probe_path}\")\n",
    "    save_model_callback()\n",
    "\n",
    "    # Final evaluation\n",
    "    eval_metrics = trainer.evaluate(\n",
    "        save_roc_curves=training_config.save_roc_curves,\n",
    "        dump_raw_eval_results=training_config.dump_raw_eval_results,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    if training_config.save_evaluation_metrics:\n",
    "        save_json(\n",
    "            eval_metrics,\n",
    "            training_config.probe_config.probe_path / \"evaluation_results.json\"\n",
    "        )\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    if training_config.upload_to_hf:\n",
    "        print(f\"Uploading probe to HuggingFace Hub...\")\n",
    "        upload_probe_to_hf(\n",
    "            repo_id=training_config.probe_config.hf_repo_id,\n",
    "            probe_id=training_config.probe_config.probe_id,\n",
    "            token=os.environ.get(\"HF_WRITE_TOKEN\"),\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18154786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config from YAML\n",
    "training_config = TrainingConfig(**load_yaml('configs/train_config_slim_llama.yaml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7e06c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mksevdari\u001b[0m (\u001b[33methz-lsai-25\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/hallucination_probes/wandb/run-20251123_134348-kmdot9i3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ethz-lsai-25/hallucination-probes/runs/kmdot9i3' target=\"_blank\">clean_code_llama3_1_8b_lora</a></strong> to <a href='https://wandb.ai/ethz-lsai-25/hallucination-probes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ethz-lsai-25/hallucination-probes' target=\"_blank\">https://wandb.ai/ethz-lsai-25/hallucination-probes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ethz-lsai-25/hallucination-probes/runs/kmdot9i3' target=\"_blank\">https://wandb.ai/ethz-lsai-25/hallucination-probes/runs/kmdot9i3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training config:\n",
      "\twandb_project: hallucination-probes\n",
      "\twandb_name: None\n",
      "\tprobe_config: {'probe_id': 'clean_code_llama3_1_8b_lora', 'model_name': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'layer': 30, 'lora_layers': [], 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'load_from': None, 'probe_path': PosixPath('/capstor/scratch/cscs/tkwiecinski/hallucination_probes/probes/clean_code_llama3_1_8b_lora'), 'hf_repo_id': 'obalcells/hallucination-probes', 'threshold': 0.5}\n",
      "\tupload_to_hf: False\n",
      "\tsave_evaluation_metrics: True\n",
      "\tsave_roc_curves: False\n",
      "\tdump_raw_eval_results: False\n",
      "\tper_device_train_batch_size: 4\n",
      "\tper_device_eval_batch_size: 4\n",
      "\thigh_loss_threshold: None\n",
      "\tlambda_lm: 0.0\n",
      "\tlambda_kl: 0.5\n",
      "\tanneal_max_aggr: True\n",
      "\tanneal_warmup: 1.0\n",
      "\tlearning_rate: 5e-05\n",
      "\tprobe_head_lr: 0.001\n",
      "\tlora_lr: 0.0001\n",
      "\tsparsity_penalty_weight: None\n",
      "\tnum_train_samples: None\n",
      "\tmax_steps: -1\n",
      "\tnum_train_epochs: 1\n",
      "\tenable_gradient_checkpointing: True\n",
      "\tgradient_accumulation_steps: 1\n",
      "\tmax_grad_norm: 1.0\n",
      "\teval_steps: None\n",
      "\tevaluation_strategy: no\n",
      "\tlogging_steps: 10\n",
      "\tseed: 42\n",
      "\ttrain_datasets: [{'dataset_id': 'llama3_1_8b_longfact_train', 'hf_repo': 'obalcells/longfact-annotations', 'subset': 'Meta-Llama-3.1-8B-Instruct', 'split': 'train', 'max_length': 1536, 'default_ignore': False, 'last_span_token': False, 'ignore_buffer': 0, 'pos_weight': 10.0, 'neg_weight': 10.0, 'shuffle': True, 'seed': 42, 'process_on_the_fly': False}]\n",
      "\teval_datasets: [{'dataset_id': 'llama3_1_8b_longform_test', 'hf_repo': 'obalcells/longfact-annotations', 'subset': 'Meta-Llama-3.1-8B-Instruct', 'split': 'test', 'max_length': 1536, 'pos_weight': 10.0, 'neg_weight': 10.0, 'default_ignore': False, 'shuffle': False}]\n",
      "\ttrain_dataset_configs: [{'dataset_id': 'llama3_1_8b_longfact_train', 'hf_repo': 'obalcells/longfact-annotations', 'subset': 'Meta-Llama-3.1-8B-Instruct', 'split': 'train', 'max_length': 1536, 'ignore_buffer': 0, 'default_ignore': False, 'last_span_token': False, 'pos_weight': 10.0, 'neg_weight': 10.0, 'shuffle': True, 'seed': 42, 'process_on_the_fly': False, 'max_num_samples': None}]\n",
      "\teval_dataset_configs: [{'dataset_id': 'llama3_1_8b_longform_test', 'hf_repo': 'obalcells/longfact-annotations', 'subset': 'Meta-Llama-3.1-8B-Instruct', 'split': 'test', 'max_length': 1536, 'ignore_buffer': 0, 'default_ignore': False, 'last_span_token': False, 'pos_weight': 10.0, 'neg_weight': 10.0, 'shuffle': False, 'seed': 42, 'process_on_the_fly': False, 'max_num_samples': None}]\n",
      "Loading model: meta-llama/Meta-Llama-3.1-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env if present\n",
    "load_dotenv()\n",
    "\n",
    "if training_config.upload_to_hf:\n",
    "    assert os.environ.get(\"HF_WRITE_TOKEN\", None) is not None\n",
    "\n",
    "wandb.init(project=training_config.wandb_project, name=training_config.probe_config.probe_id)\n",
    "\n",
    "print(\"Training config:\")\n",
    "for key, value in asdict(training_config).items():\n",
    "    print(f\"\\t{key}: {value}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "print(f\"Loading model: {training_config.probe_config.model_name}\")\n",
    "model, tokenizer = load_model_and_tokenizer(\n",
    "    training_config.probe_config.model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4964565e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73456338",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(model, 'config'):\n",
    "    try:\n",
    "        model.config.use_cache = False\n",
    "    except Exception:\n",
    "        pass\n",
    "if training_config.enable_gradient_checkpointing and hasattr(model, 'gradient_checkpointing_enable'):\n",
    "    try:\n",
    "        model.gradient_checkpointing_enable()\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8f3aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up probe: clean_code_llama3_1_8b_lora\n",
      "WARNING: Model is not a PeftModel. Remember to add LoRA adapters if needed.\n",
      "WARNING: Using seed=42 for the initialization of the probe\n",
      "Parameters that will be trained:\n",
      "  - value_head.weight: shape torch.Size([1, 4096]), device cuda:0\n",
      "  - value_head.bias: shape torch.Size([1]), device cuda:0\n",
      "\n",
      "Total trainable parameters: 4,097 (0.00%)\n",
      "Total parameters: 8,030,265,345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4097, 8030265345)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Setting up probe: {training_config.probe_config.probe_id}\")\n",
    "model, probe = setup_probe(model, training_config.probe_config)\n",
    "\n",
    "print_trainable_parameters(probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7818f199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ValueHeadProbe(\n",
       "  (model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(128256, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "  )\n",
       "  (target_module): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (value_head): Linear(in_features=4096, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db88c425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19de8309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProbeConfig(probe_id='clean_code_llama3_1_8b_lora', model_name='meta-llama/Meta-Llama-3.1-8B-Instruct', layer=30, lora_layers=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30], lora_r=16, lora_alpha=32, lora_dropout=0.05, load_from=None, probe_path=PosixPath('/capstor/scratch/cscs/tkwiecinski/hallucination_probes/probes/clean_code_llama3_1_8b_lora'), hf_repo_id='obalcells/hallucination-probes', threshold=0.5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_config.probe_config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
